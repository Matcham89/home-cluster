---
# yaml-language-server: $schema=https://raw.githubusercontent.com/fluxcd-community/flux2-schemas/main/helmrelease-helm-v2.json
apiVersion: helm.toolkit.fluxcd.io/v2
kind: HelmRelease
metadata:
  name: kube-prometheus-stack
  namespace: monitoring
spec:
  interval: 1m
  chartRef:
    kind: OCIRepository
    name: kube-prometheus-stack
  install:
    strategy:
      name: RetryOnFailure
    createNamespace: true
    timeout: 10m
    crds: Create
  upgrade:
    strategy:
      name: RetryOnFailure
    cleanupOnFail: true
    timeout: 10m
    crds: CreateReplace
  valuesFrom:
  - kind: ConfigMap
    name: flux-kube-state-metrics-config
    valuesKey: kube-state-metrics-config.yaml
  values:
    alertmanager:
      enabled: true
      service:
        type: LoadBalancer
        loadBalancerIP: "192.168.1.203"
      alertmanagerSpec:
        resources:
          requests:
            cpu: 50m
            memory: 128Mi
          limits:
            cpu: 200m
            memory: 256Mi
    kubeStateMetrics:
      enabled: true
      service:
        type: LoadBalancer
        loadBalancerIP: "192.168.1.204"
    kube-state-metrics:
      resources:
        requests:
          cpu: 50m
          memory: 64Mi
        limits:
          cpu: 200m
          memory: 256Mi
      prometheus:
        monitor:
          enabled: true
          interval: 30s
          scrapeTimeout: 30s
    nodeExporter:
      enabled: true
      operatingSystems:
        linux:
          enabled: true
    prometheus-node-exporter:
      resources:
        requests:
          cpu: 50m
          memory: 64Mi
        limits:
          cpu: 200m
          memory: 256Mi
    prometheus:
      enabled: true
      service:
        type: LoadBalancer
        loadBalancerIP: "192.168.1.208"
      prometheusSpec:
        resources:
          requests:
            cpu: 200m
            memory: 1Gi
          limits:
            cpu: 1000m
            memory: 2Gi
        ruleSelectorNilUsesHelmValues: false
        serviceMonitorSelectorNilUsesHelmValues: false
        podMonitorSelectorNilUsesHelmValues: false
        enableFeatures:
          - native-histograms
        enableRemoteWriteReceiver: true
        additionalScrapeConfigs:
        - job_name: 'postgres-external'
          static_configs:
            - targets: ['postgres-exporter-external.external-services.svc.cluster.local:9187']
              labels:
                service: 'postgres-external'
                instance: 'external-db'
    grafana:
      enabled: true
      grafana.ini:
        server:
          root_url: "https://grafana.kubegit.com/"
        auth:
          signout_redirect_url: "https://authentik.kubegit.com/application/o/grafana-kubegit-com/end-session/"
          oauth_auto_login: true
        auth.generic_oauth:
          name: authentik
          enabled: true
          client_id: $__file{/etc/secrets/auth_generic_oauth/client_id}
          client_secret: $__file{/etc/secrets/auth_generic_oauth/client_secret}
          scopes: "openid profile email"
          auth_url: "https://authentik.kubegit.com/application/o/authorize/"
          token_url: "https://authentik.kubegit.com/application/o/token/"
          api_url: "https://authentik.kubegit.com/application/o/userinfo/"
          # Optionally map user groups to Grafana roles
          role_attribute_path: contains(groups, 'Grafana Admins') && 'Admin' || contains(groups, 'Grafana Editors') && 'Editor' || 'Viewer'
      extraSecretMounts:
        - name: auth-generic-oauth-secret-mount
          secretName: auth-generic-oauth-secret
          defaultMode: 0440
          mountPath: /etc/secrets/auth_generic_oauth
          readOnly: true
      service:
        type: LoadBalancer
        loadBalancerIP: "192.168.1.209"
      defaultDashboardsEnabled: false
      defaultDashboardsTimezone: mdt
      persistence:
        enabled: false
        type: pvc
        size: 10Gi
      datasources:
        datasources.yaml:
          apiVersion: 1
          datasources:
            - name: Prometheus
              type: prometheus
              uid: prometheus
              access: proxy
              orgId: 1
              url: http://kube-prometheus-stack-prometheus.monitoring:9090
              basicAuth: false
              editable: true
              isDefault: false
              jsonData:
                httpMethod: GET
                exemplarTraceIdDestinations:
                - name: trace_id
                  datasourceUid: tempo
            - name: Tempo
              type: tempo
              access: proxy
              basicAuth: false
              orgId: 1
              uid: tempo
              url: http://tempo.monitoring.svc.cluster.local:3200
              isDefault: false
              editable: true
            - orgId: 1
              name: Loki
              type: loki
              typeName: Loki
              access: proxy
              url: http://loki.monitoring.svc.cluster.local:3100
              basicAuth: false
              isDefault: false
              editable: true
      sidecar:
        dashboards:
          enabled: true
          label: grafana_dashboard
          labelValue: "1"
          folder: /tmp/dashboards
          searchNamespace: monitoring
          provider:
            foldersFromFilesStructure: true
          resources:
            requests:
              cpu: 50m
              memory: 64Mi
            limits:
              cpu: 200m
              memory: 256Mi
        datasources:
          resources:
            requests:
              cpu: 50m
              memory: 64Mi
            limits:
              cpu: 200m
              memory: 256Mi
      resources:
        requests:
          cpu: 100m
          memory: 256Mi
        limits:
          cpu: 500m
          memory: 512Mi
    prometheusOperator:
      resources:
        requests:
          cpu: 50m
          memory: 128Mi
        limits:
          cpu: 200m
          memory: 256Mi
